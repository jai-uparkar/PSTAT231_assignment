---
title: "Homework 6"
author: "Jai Uparkar"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
library(knitr)
library(rsample)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(ggthemes)
library(recipes)
library(corrr)
library(corrplot)
library(dplyr)
library(yardstick)
library(discrim)
library(klaR)
library(janitor)
library(rpart)
library(rpart.plot)
library(ranger)
library(randomForest)
library(parsnip)
library(vip)

setwd("/Users/jaiuparkar/Downloads/PSTAT231/HW6")

knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

### Exercise 1

Read in the data and set things up as in Homework 5:

- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r, results='hide'}
removeP<- c('Bug', 'Fire', 'Grass', 'Normal', 'Water', 'Psychic')

pokemon_df <- read_csv("data/Pokemon.csv") %>% 
  clean_names() %>% 
  filter(type_1 %in% removeP) %>% 
  mutate(type_1 = factor(type_1), legendary = factor(legendary))

set.seed(789)
pokemon_split <- initial_split(pokemon_df, prop = 0.7,strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

pokemon_cv <- vfold_cv(pokemon_train, strata = type_1, v = 5)

pokemon_recipe <- recipe(type_1 ~ legendary + generation + 
                           sp_atk + attack + speed + defense + 
                           hp + sp_def, data = pokemon_train)  %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

What relationships, if any, do you notice? Do these relationships make sense to you?

```{r, results='hide'}
pokemon_train %>% 
  dplyr::select(is.numeric, -c(number)) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(type = "lower")
```

To make my correlation plot, I only included numeric variables and also removed the variable `number` since it refers to an id number (which is categorical) but micmics much of the same information as the name of the pokemon. In the correlation plot, I noticed that the variable is positively correlated with hp, attack, defense, sp_atk, sp_def, and speed (especially with sp_atk and sp_def). There isn't any negative correlation in the matrix.  These relationships make sense to me because the better defense, attack, or any other attribute a Pokemon has, the more powerful it is, especially since it's a sum of all these variables. 

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r, results='hide'}
pokemon_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tree_pokemon <- decision_tree() %>%
  set_engine("rpart")%>%
  set_mode("classification") %>% 
  set_args(cost_complexity = tune())

pokemon_tree_workflow <- workflow() %>% 
  add_model(tree_pokemon) %>% 
  add_recipe(pokemon_recipe)

tune_tree_pokemon <- tune_grid(pokemon_tree_workflow, 
  resamples = pokemon_cv, 
  grid = pokemon_grid, 
  metrics = metric_set(roc_auc))

autoplot(tune_tree_pokemon)

```

From the autoplot, I observed a downward trend as the cost complexity parameter increased with a peak of 0.645 around a cost-complexity of 0.010. A single decision tree, from the plot, performs better with a smaller cost-complexity parameter, but the best with a medium level cost (0.010). 

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*
```{r, results='hide'}
collect_metrics(tune_tree_pokemon) %>% 
  arrange(-mean)  %>% 
  dplyr::slice(1)

```

The roc_auc of my best-performing pruned decision tree on the folds is 0.648. It has a `cost_complexity` of .0129 which matches up with our observations from the auto plot we generated previously (a small cost value).

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r, results='hide'}
best_tree <- select_best(tune_tree_pokemon)
pokemon_tree_final <- finalize_workflow(pokemon_tree_workflow, best_tree)

pokemon_tree_finalfit <- fit(pokemon_tree_final , data = pokemon_train)

pokemon_tree_finalfit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = F)

```

### Exercise 6

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r, results='hide'}
rf_grid <- grid_regular(mtry(range =c(1, 8)), 
                           trees(range =c(1, 100)),
                           min_n(range =c(1, 10)),
                           levels = 8)

rf_model <- rand_forest(mtry = tune(), 
                       trees = tune(),
                       min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

pokemon_rf <- workflow() %>% 
  add_model(rf_model ) %>% 
  add_recipe(pokemon_recipe)
```

`mtry`: the number of predictors randomly picked when a split occurs
`trees`: total number of trees contained in the model
`min_n`: least amount of data records in a node needed before splitting the node again

`mtry` should not be smaller than 1 because otherwise its possible our model will contain no predictors which is does not make sense. Also since we have a total of 8 predictors in our model, we can't have any more than that so that will also not be possible. 

### Exercise 7

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r, results='hide'}

# ran this previously and it takes a while but here is the code thatI used to generate my tuning
# model
#tune_rf <- tune_grid(pokemon_rf, 
  #resamples = pokemon_cv, 
  #grid = rf_grid, 
  #metrics = metric_set(roc_auc))

#save(tune_rf, file = 'rf_pokemon.RData')
```

```{r, results='hide'}
load('rf_pokemon.RData')
autoplot(tune_rf)
```

From the `autplot()` function, I notice that the models perform roughly the same regardless of how many minimal node sizes there are. Also it's very clear that when you have more trees the model performs poorly as shows by the red line in every graph. When the number of trees  are between 48 and 85, that's when the `roc_auc` performs it's best although its hard to see which one is ultimately the best because they are tightly overlapping. If I had to choose though, the best model would be when there are 2 predictors, a node size of 7, and 71 trees.  

### Exercise 8

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r, echo = TRUE}
collect_metrics(tune_rf) %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)
```

The `roc_auc` of my best-performing random forest model on the folds is 0.725 from prepocessor1_model483 which has a `min_n` = 10, `trees = 57`, `mtry` = 3. 

### Exercise 9

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

```{r, echo = TRUE}

best_rf <- select_best(tune_rf)
pokemon_rf_final <- finalize_workflow(pokemon_rf , best_rf)
pokemon_rf_finalfit <- fit(pokemon_rf_final , data = pokemon_train)

pokemon_rf_finalfit%>%
  extract_fit_engine() %>%
  vip() 
```

From the variable importance plot, the variables that were the most important were so_atk, attack, hp, and speed. The least important variables were legendary_True and generation. The variables that are important make sense because they were highly correlated with total, and the specific attributes of a pokemon is what makes it uniquely identifiable. 

### Exercise 10

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

```{r, echo = TRUE}

bt_grid <- grid_regular(trees(range=c(10, 2000)), levels = 10)

bt_model <- boost_tree(trees = tune()) %>% 
  set_engine("xgboost") %>%
  set_mode("classification")

bt_workflow <- workflow() %>% 
  add_model(bt_model) %>% 
  add_recipe(pokemon_recipe)
```

```{r, echo = TRUE}
#tune_bt <- tune_grid(bt_workflow, 
  #resamples = pokemon_cv,
  ##grid = bt_grid,
  #metrics = metric_set(roc_auc))

#save(tune_bt, file = 'bt_pokemon.RData')

```

What do you observe?

```{r, echo = TRUE}
load('bt_pokemon.RData')
autoplot(tune_bt)
```

From the `autoplot` function, we observe a downward trend as the number of trees increase with a slight uptick after 1300 trees. The optimal number of trees is slightly less than 250 and and it achieves an `roc_auc` of .677 which is worse compared to the boosted tree model. 

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r, echo = TRUE}

collect_metrics(tune_bt) %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)
```

The `roc_auc` of my best-performing random forest model on the folds is 0.677 from prepocessor1_model02 where `trees` = 231. 

### Exercise 11

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

```{r, echo = TRUE}

best_bt <- select_best(tune_bt)
pokemon_bt_final <- finalize_workflow(bt_workflow , best_bt)
pokemon_bt_finalfit <- fit(pokemon_bt_final , data = pokemon_train)

best_rf <- select_best(tune_rf)
pokemon_rf_final <- finalize_workflow(pokemon_rf, best_rf)
pokemon_rf_finalfit <- fit(pokemon_rf_final , data = pokemon_train)
rf_fit_results <- fit(pokemon_rf_finalfit, pokemon_train)

roc_auc<- c(0.6484917, 0.7249049, 0.6774798)
Method<- c('Decision Tree', 'Random Forest', 'Boosted Tree')
table_methods<- data.frame(Method, roc_auc)
table_methods
```

The method that performed the best was the random forest as seen in the table 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

```{r, echo = TRUE}
tree_fit_results <- fit(pokemon_tree_finalfit, pokemon_train)
predicted_tree <- augment(tree_fit_results , new_data = pokemon_test) %>%
  dplyr::select(type_1, starts_with(".pred"))

predicted_tree %>% roc_auc(type_1, .pred_Bug:.pred_Water)
```

```{r, echo = TRUE}
predicted_tree %>% roc_curve(type_1, .pred_Bug:.pred_Water) %>% 
  autoplot()
```

```{r, echo = TRUE}
predicted_tree %>% 
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

Which classes was your model most accurate at predicting? Which was it worst at?

From the roc_auc plot, our model was the most accurate at predicting Psychic, Normal, and Fire. Even then these models are not the best but their plots most resemble an ideal roc_auc plot with an upside down L shape. From the heatmap we can see that the same is true since the Psychic type correctly predicts itself the most. However, it also classifies itself as Normal a decent amount. The model is worst at predicting water since it predicts all the other classes as much as it predicts itself correctly. 

## For 231 Students

### Exercise 12

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?

```{r, echo = TRUE}
set.seed(789)
abalone_df<- read.csv('data/abalone.csv')
abalone_df <- abalone_df %>% 
  mutate(age = rings + 1.5)

abalone_split <- initial_split(abalone_df, prop = 0.7,
                                strata = age)
abalone_train <- training(abalone_split)
abalone_test <- testing(abalone_split)

abalone_cv <- vfold_cv(abalone_train, strata = age, v = 5)

```

```{r, echo = TRUE}
abalone_recipe <- recipe(age~., data = abalone_train) %>% 
  step_rm(rings) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())

```

```{r, echo = TRUE}
rf_grid <- grid_regular(mtry(range =c(1, 8)), 
                           trees(range =c(1, 100)),
                           min_n(range =c(1, 10)),
                           levels = 5)

rf_model <- rand_forest(mtry = tune(), 
                       trees = tune(),
                       min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

abalone_rf <- workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(abalone_recipe)
```

```{r, echo = TRUE}
#tune_rf_abalone <- tune_grid(abalone_rf, 
 # resamples = abalone_cv, 
  #grid = rf_grid, 
  #metrics = metric_set(rmse))

#save(tune_rf_abalone, file = 'rf_abalone.RData')
```

```{r, echo = TRUE}
load('rf_abalone.RData')

# Metrics for the best model on training
collect_metrics(tune_rf_abalone) %>% 
  arrange(-mean) %>% 
  dplyr::slice(1)

```

```{r, echo = TRUE}
abalone_best_rf <- select_best(tune_rf_abalone)
abalone_rf_final <- finalize_workflow(abalone_rf , abalone_best_rf)
abalone_rf_finalfit <- fit(abalone_rf_final , data = abalone_train)

predicted_rf <- augment(abalone_rf_finalfit , new_data = abalone_test) %>%
  dplyr::select(age, ".pred")

predicted_rf %>% rmse(age, .pred)
```
The model's RMSE on my testing data set was 2.23. The smaller the RMSE value is, the more accurate the model is. However this is all relative to what are the actual value we are predicting. In this case, the range of an abalone's age in this dataset is bewteen 2.5 and 30.5 and even though our RMSE seems very large at first, we can see that relative to the range it is not that bad, although it can be much better. 
